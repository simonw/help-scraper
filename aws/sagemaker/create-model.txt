CREATE-MODEL()                                                  CREATE-MODEL()



NAME
       create-model -

DESCRIPTION
       Creates a model in Amazon SageMaker. In the request, you name the model
       and describe a primary container. For the primary container, you  spec-
       ify  the  Docker  image  that  contains inference code, artifacts (from
       prior training), and a custom environment map that the  inference  code
       uses when you deploy the model for predictions.

       Use  this  API  to  create  a model if you want to use Amazon SageMaker
       hosting services or run a batch transform job.

       To host your model, you create an endpoint configuration with the  Cre-
       ateEndpointConfig  API, and then create an endpoint with the CreateEnd-
       point API. Amazon SageMaker then deploys all of the containers that you
       defined for the model in the hosting environment.

       For  an example that calls this method when deploying a model to Amazon
       SageMaker hosting services, see Deploy the Model  to  Amazon  SageMaker
       Hosting Services (Amazon Web Services SDK for Python (Boto 3)).

       To  run  a  batch  transform using your model, you start a job with the
       CreateTransformJob API. Amazon  SageMaker  uses  your  model  and  your
       dataset  to get inferences which are then saved to a specified S3 loca-
       tion.

       In the CreateModel request, you must define a container with the Prima-
       ryContainer parameter.

       In  the request, you also provide an IAM role that Amazon SageMaker can
       assume to access model artifacts and docker image for deployment on  ML
       compute hosting instances or for batch transform jobs. In addition, you
       also use the IAM role to manage permissions the inference  code  needs.
       For example, if the inference code access any other Amazon Web Services
       resources, you grant necessary permissions via this role.

       See also: AWS API Documentation

       See 'aws help' for descriptions of global parameters.

SYNOPSIS
            create-model
          --model-name <value>
          [--primary-container <value>]
          [--containers <value>]
          [--inference-execution-config <value>]
          --execution-role-arn <value>
          [--tags <value>]
          [--vpc-config <value>]
          [--enable-network-isolation | --no-enable-network-isolation]
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]

OPTIONS
       --model-name (string)
          The name of the new model.

       --primary-container (structure)
          The location of the primary docker image containing inference  code,
          associated  artifacts, and custom environment map that the inference
          code uses when the model is deployed for predictions.

          ContainerHostname -> (string)
              This parameter is ignored for models that contain only a  Prima-
              ryContainer .

              When a ContainerDefinition is part of an inference pipeline, the
              value of the parameter uniquely identifies the container for the
              purposes  of  logging and metrics. For information, see Use Logs
              and Metrics to Monitor an Inference  Pipeline  .  If  you  don't
              specify  a  value  for  this parameter for a ContainerDefinition
              that is part of an inference pipeline, a unique name is automat-
              ically assigned based on the position of the ContainerDefinition
              in the pipeline. If you specify a value for  the  ContainerHost-
              Name  for  any  ContainerDefinition that is part of an inference
              pipeline, you must specify a value for the ContainerHostName pa-
              rameter of every ContainerDefinition in that pipeline.

          Image -> (string)
              The  path  where inference code is stored. This can be either in
              Amazon EC2 Container Registry or in a Docker  registry  that  is
              accessible  from  the  same VPC that you configure for your end-
              point. If you are using your own custom algorithm instead of  an
              algorithm  provided by Amazon SageMaker, the inference code must
              meet Amazon SageMaker requirements.  Amazon  SageMaker  supports
              both  registry/repository[:tag] and registry/repository[@digest]
              image path formats. For more information, see Using Your Own Al-
              gorithms with Amazon SageMaker

          ImageConfig -> (structure)
              Specifies whether the model container is in Amazon ECR or a pri-
              vate Docker registry accessible from your Amazon Virtual Private
              Cloud  (VPC). For information about storing containers in a pri-
              vate Docker registry, see Use  a  Private  Docker  Registry  for
              Real-Time Inference Containers

              RepositoryAccessMode -> (string)
                 Set this to one of the following values:

                 o Platform - The model image is hosted in Amazon ECR.

                 o Vpc  -  The  model image is hosted in a private Docker reg-
                   istry in your VPC.

              RepositoryAuthConfig -> (structure)
                 (Optional) Specifies an authentication configuration for  the
                 private  docker  registry  where  your model image is hosted.
                 Specify a value for this property only if you  specified  Vpc
                 as the value for the RepositoryAccessMode field, and the pri-
                 vate Docker registry where the model image is hosted requires
                 authentication.

                 RepositoryCredentialsProviderArn -> (string)
                     The  Amazon Resource Name (ARN) of an Amazon Web Services
                     Lambda function that provides credentials to authenticate
                     to  the private Docker registry where your model image is
                     hosted. For information about how to create an Amazon Web
                     Services  Lambda  function,  see Create a Lambda function
                     with the console in the Amazon Web Services Lambda Devel-
                     oper Guide .

          Mode -> (string)
              Whether the container hosts a single model or multiple models.

          ModelDataUrl -> (string)
              The  S3  path where the model artifacts, which result from model
              training, are stored. This path must point to a single gzip com-
              pressed  tar  archive  (.tar.gz suffix). The S3 path is required
              for Amazon SageMaker built-in algorithms, but  not  if  you  use
              your  own  algorithms.  For  more  information on built-in algo-
              rithms, see Common Parameters .

              NOTE:
                 The model artifacts must be in an S3 bucket that  is  in  the
                 same region as the model or endpoint you are creating.

              If you provide a value for this parameter, Amazon SageMaker uses
              Amazon Web Services Security Token Service to download model ar-
              tifacts from the S3 path you provide. Amazon Web Services STS is
              activated in your IAM user account by default. If you previously
              deactivated  Amazon  Web  Services STS for a region, you need to
              reactivate Amazon Web Services STS for that region. For more in-
              formation,  see  Activating and Deactivating Amazon Web Services
              STS in an Amazon Web Services Region in the Amazon Web  Services
              Identity and Access Management User Guide .

              WARNING:
                 If  you  use  a  built-in algorithm to create a model, Amazon
                 SageMaker requires that you provide a S3 path  to  the  model
                 artifacts in ModelDataUrl .

          Environment -> (map)
              The  environment  variables to set in the Docker container. Each
              key and value in the Environment string to string map  can  have
              length of up to 1024. We support up to 16 entries in the map.

              key -> (string)

              value -> (string)

          ModelPackageName -> (string)
              The  name  or Amazon Resource Name (ARN) of the model package to
              use to create the model.

          InferenceSpecificationName -> (string)
              The inference specification name in the model package version.

          MultiModelConfig -> (structure)
              Specifies additional configuration for multi-model endpoints.

              ModelCacheSetting -> (string)
                 Whether to cache models for a multi-model  endpoint.  By  de-
                 fault,  multi-model  endpoints  cache  models so that a model
                 does not have to be loaded into memory each time  it  is  in-
                 voked.  Some use cases do not benefit from model caching. For
                 example, if an endpoint hosts a large number of  models  that
                 are  each  invoked  infrequently,  the endpoint might perform
                 better  if  you  disable  model  caching.  To  disable  model
                 caching, set the value of this parameter to Disabled .

       Shorthand Syntax:

          ContainerHostname=string,Image=string,ImageConfig={RepositoryAccessMode=string,RepositoryAuthConfig={RepositoryCredentialsProviderArn=string}},Mode=string,ModelDataUrl=string,Environment={KeyName1=string,KeyName2=string},ModelPackageName=string,InferenceSpecificationName=string,MultiModelConfig={ModelCacheSetting=string}

       JSON Syntax:

          {
            "ContainerHostname": "string",
            "Image": "string",
            "ImageConfig": {
              "RepositoryAccessMode": "Platform"|"Vpc",
              "RepositoryAuthConfig": {
                "RepositoryCredentialsProviderArn": "string"
              }
            },
            "Mode": "SingleModel"|"MultiModel",
            "ModelDataUrl": "string",
            "Environment": {"string": "string"
              ...},
            "ModelPackageName": "string",
            "InferenceSpecificationName": "string",
            "MultiModelConfig": {
              "ModelCacheSetting": "Enabled"|"Disabled"
            }
          }

       --containers (list)
          Specifies the containers in the inference pipeline.

          (structure)
              Describes the container, as part of model definition.

              ContainerHostname -> (string)
                 This parameter is ignored for models that contain only a Pri-
                 maryContainer .

                 When a ContainerDefinition is part of an inference  pipeline,
                 the  value of the parameter uniquely identifies the container
                 for the purposes of logging and metrics. For information, see
                 Use  Logs  and  Metrics to Monitor an Inference Pipeline . If
                 you don't specify a value for this parameter for  a  Contain-
                 erDefinition  that is part of an inference pipeline, a unique
                 name is automatically assigned based on the position  of  the
                 ContainerDefinition  in  the pipeline. If you specify a value
                 for the ContainerHostName for any ContainerDefinition that is
                 part  of  an inference pipeline, you must specify a value for
                 the ContainerHostName parameter of every  ContainerDefinition
                 in that pipeline.

              Image -> (string)
                 The  path  where inference code is stored. This can be either
                 in Amazon EC2 Container Registry or in a Docker registry that
                 is  accessible  from the same VPC that you configure for your
                 endpoint. If you are using your own custom algorithm  instead
                 of  an  algorithm provided by Amazon SageMaker, the inference
                 code must meet Amazon SageMaker  requirements.  Amazon  Sage-
                 Maker   supports   both  registry/repository[:tag]  and  reg-
                 istry/repository[@digest] image path formats. For more infor-
                 mation, see Using Your Own Algorithms with Amazon SageMaker

              ImageConfig -> (structure)
                 Specifies  whether  the model container is in Amazon ECR or a
                 private Docker registry accessible from your  Amazon  Virtual
                 Private Cloud (VPC). For information about storing containers
                 in a private Docker registry, see Use a Private  Docker  Reg-
                 istry for Real-Time Inference Containers

                 RepositoryAccessMode -> (string)
                     Set this to one of the following values:

                     o Platform - The model image is hosted in Amazon ECR.

                     o Vpc  -  The  model  image is hosted in a private Docker
                       registry in your VPC.

                 RepositoryAuthConfig -> (structure)
                     (Optional) Specifies an authentication configuration  for
                     the  private  docker  registry  where your model image is
                     hosted. Specify a value for this  property  only  if  you
                     specified  Vpc  as the value for the RepositoryAccessMode
                     field, and the private Docker registry  where  the  model
                     image is hosted requires authentication.

                     RepositoryCredentialsProviderArn -> (string)
                        The  Amazon  Resource Name (ARN) of an Amazon Web Ser-
                        vices Lambda function that provides credentials to au-
                        thenticate  to  the private Docker registry where your
                        model image is hosted. For information  about  how  to
                        create  an  Amazon  Web  Services Lambda function, see
                        Create a Lambda function with the console in the  Ama-
                        zon Web Services Lambda Developer Guide .

              Mode -> (string)
                 Whether  the  container hosts a single model or multiple mod-
                 els.

              ModelDataUrl -> (string)
                 The S3 path where the  model  artifacts,  which  result  from
                 model  training, are stored. This path must point to a single
                 gzip compressed tar archive (.tar.gz suffix). The S3 path  is
                 required for Amazon SageMaker built-in algorithms, but not if
                 you use your own algorithms. For more information on built-in
                 algorithms, see Common Parameters .

                 NOTE:
                     The  model  artifacts  must be in an S3 bucket that is in
                     the same region as the model or endpoint you  are  creat-
                     ing.

                 If  you  provide a value for this parameter, Amazon SageMaker
                 uses Amazon Web Services Security Token Service  to  download
                 model artifacts from the S3 path you provide. Amazon Web Ser-
                 vices STS is activated in your IAM user account  by  default.
                 If  you  previously deactivated Amazon Web Services STS for a
                 region, you need to reactivate Amazon Web  Services  STS  for
                 that region. For more information, see Activating and Deacti-
                 vating Amazon Web Services STS in an Amazon Web Services  Re-
                 gion  in  the Amazon Web Services Identity and Access Manage-
                 ment User Guide .

                 WARNING:
                     If you use a built-in algorithm to create a model, Amazon
                     SageMaker  requires  that  you  provide  a S3 path to the
                     model artifacts in ModelDataUrl .

              Environment -> (map)
                 The environment variables to set  in  the  Docker  container.
                 Each  key  and  value in the Environment string to string map
                 can have length of up to 1024. We support up to 16 entries in
                 the map.

                 key -> (string)

                 value -> (string)

              ModelPackageName -> (string)
                 The  name  or Amazon Resource Name (ARN) of the model package
                 to use to create the model.

              InferenceSpecificationName -> (string)
                 The inference specification name in the  model  package  ver-
                 sion.

              MultiModelConfig -> (structure)
                 Specifies additional configuration for multi-model endpoints.

                 ModelCacheSetting -> (string)
                     Whether  to  cache  models for a multi-model endpoint. By
                     default, multi-model endpoints cache  models  so  that  a
                     model does not have to be loaded into memory each time it
                     is invoked. Some use cases  do  not  benefit  from  model
                     caching. For example, if an endpoint hosts a large number
                     of models that are each invoked  infrequently,  the  end-
                     point  might perform better if you disable model caching.
                     To disable model caching, set the value of this parameter
                     to Disabled .

       Shorthand Syntax:

          ContainerHostname=string,Image=string,ImageConfig={RepositoryAccessMode=string,RepositoryAuthConfig={RepositoryCredentialsProviderArn=string}},Mode=string,ModelDataUrl=string,Environment={KeyName1=string,KeyName2=string},ModelPackageName=string,InferenceSpecificationName=string,MultiModelConfig={ModelCacheSetting=string} ...

       JSON Syntax:

          [
            {
              "ContainerHostname": "string",
              "Image": "string",
              "ImageConfig": {
                "RepositoryAccessMode": "Platform"|"Vpc",
                "RepositoryAuthConfig": {
                  "RepositoryCredentialsProviderArn": "string"
                }
              },
              "Mode": "SingleModel"|"MultiModel",
              "ModelDataUrl": "string",
              "Environment": {"string": "string"
                ...},
              "ModelPackageName": "string",
              "InferenceSpecificationName": "string",
              "MultiModelConfig": {
                "ModelCacheSetting": "Enabled"|"Disabled"
              }
            }
            ...
          ]

       --inference-execution-config (structure)
          Specifies  details  of  how containers in a multi-container endpoint
          are called.

          Mode -> (string)
              How containers in a multi-container are run. The following  val-
              ues are valid.

              o SERIAL - Containers run as a serial pipeline.

              o DIRECT  -  Only  the  individual container that you specify is
                run.

       Shorthand Syntax:

          Mode=string

       JSON Syntax:

          {
            "Mode": "Serial"|"Direct"
          }

       --execution-role-arn (string)
          The Amazon Resource Name (ARN) of the IAM role that Amazon SageMaker
          can assume to access model artifacts and docker image for deployment
          on ML compute instances or for batch transform jobs. Deploying on ML
          compute  instances  is  part of model hosting. For more information,
          see Amazon SageMaker Roles .

          NOTE:
              To be able to pass this role to Amazon SageMaker, the caller  of
              this API must have the iam:PassRole permission.

       --tags (list)
          An  array  of  key-value  pairs. You can use tags to categorize your
          Amazon Web Services resources in different  ways,  for  example,  by
          purpose,  owner,  or  environment. For more information, see Tagging
          Amazon Web Services Resources .

          (structure)
              A tag object that consists of a key and an optional value,  used
              to manage metadata for SageMaker Amazon Web Services resources.

              You  can add tags to notebook instances, training jobs, hyperpa-
              rameter tuning jobs,  batch  transform  jobs,  models,  labeling
              jobs,  work  teams,  endpoint configurations, and endpoints. For
              more information on adding tags to SageMaker resources, see  Ad-
              dTags .

              For  more information on adding metadata to your Amazon Web Ser-
              vices resources with tagging, see Tagging  Amazon  Web  Services
              resources . For advice on best practices for managing Amazon Web
              Services resources with tagging, see Tagging Best Practices: Im-
              plement an Effective Amazon Web Services Resource Tagging Strat-
              egy .

              Key -> (string)
                 The tag key. Tag keys must be unique per resource.

              Value -> (string)
                 The tag value.

       Shorthand Syntax:

          Key=string,Value=string ...

       JSON Syntax:

          [
            {
              "Key": "string",
              "Value": "string"
            }
            ...
          ]

       --vpc-config (structure)
          A  VpcConfig object that specifies the VPC that you want your  model
          to  connect  to.  Control access to and from your model container by
          configuring the VPC. VpcConfig is used in hosting  services  and  in
          batch  transform. For more information, see Protect Endpoints by Us-
          ing an Amazon Virtual Private Cloud and Protect Data in Batch Trans-
          form Jobs by Using an Amazon Virtual Private Cloud .

          SecurityGroupIds -> (list)
              The VPC security group IDs, in the form sg-xxxxxxxx. Specify the
              security groups for the VPC that is  specified  in  the  Subnets
              field.

              (string)

          Subnets -> (list)
              The  ID  of  the subnets in the VPC to which you want to connect
              your training job or model. For information about the availabil-
              ity of specific instance types, see Supported Instance Types and
              Availability Zones .

              (string)

       Shorthand Syntax:

          SecurityGroupIds=string,string,Subnets=string,string

       JSON Syntax:

          {
            "SecurityGroupIds": ["string", ...],
            "Subnets": ["string", ...]
          }

       --enable-network-isolation | --no-enable-network-isolation (boolean)
          Isolates the model container. No inbound or outbound  network  calls
          can be made to or from the model container.

       --cli-input-json  (string) Performs service operation based on the JSON
       string provided. The JSON string follows the format provided by  --gen-
       erate-cli-skeleton.  If  other  arguments  are  provided on the command
       line, the CLI values will override the JSON-provided values. It is  not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for  --cli-input-json.  If provided with the value output, it validates
       the command inputs and returns a sample output JSON for that command.

       See 'aws help' for descriptions of global parameters.

OUTPUT
       ModelArn -> (string)
          The ARN of the model created in Amazon SageMaker.



                                                                CREATE-MODEL()
