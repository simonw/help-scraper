DESCRIBE-MODEL()                                              DESCRIBE-MODEL()



NAME
       describe-model -

DESCRIPTION
       Describes a model that you created using the CreateModel API.

       See also: AWS API Documentation

       See 'aws help' for descriptions of global parameters.

SYNOPSIS
            describe-model
          --model-name <value>
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]

OPTIONS
       --model-name (string)
          The name of the model.

       --cli-input-json  (string) Performs service operation based on the JSON
       string provided. The JSON string follows the format provided by  --gen-
       erate-cli-skeleton.  If  other  arguments  are  provided on the command
       line, the CLI values will override the JSON-provided values. It is  not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for  --cli-input-json.  If provided with the value output, it validates
       the command inputs and returns a sample output JSON for that command.

       See 'aws help' for descriptions of global parameters.

OUTPUT
       ModelName -> (string)
          Name of the SageMaker model.

       PrimaryContainer -> (structure)
          The location of the primary inference  code,  associated  artifacts,
          and  custom  environment map that the inference code uses when it is
          deployed in production.

          ContainerHostname -> (string)
              This parameter is ignored for models that contain only a  Prima-
              ryContainer .

              When a ContainerDefinition is part of an inference pipeline, the
              value of the parameter uniquely identifies the container for the
              purposes  of  logging and metrics. For information, see Use Logs
              and Metrics to Monitor an Inference  Pipeline  .  If  you  don't
              specify  a  value  for  this parameter for a ContainerDefinition
              that is part of an inference pipeline, a unique name is automat-
              ically assigned based on the position of the ContainerDefinition
              in the pipeline. If you specify a value for  the  ContainerHost-
              Name  for  any  ContainerDefinition that is part of an inference
              pipeline, you must specify a value for the ContainerHostName pa-
              rameter of every ContainerDefinition in that pipeline.

          Image -> (string)
              The  path  where inference code is stored. This can be either in
              Amazon EC2 Container Registry or in a Docker  registry  that  is
              accessible  from  the  same VPC that you configure for your end-
              point. If you are using your own custom algorithm instead of  an
              algorithm  provided  by  SageMaker, the inference code must meet
              SageMaker requirements. SageMaker supports both registry/reposi-
              tory[:tag]  and registry/repository[@digest] image path formats.
              For more information, see Using Your Own Algorithms with  Amazon
              SageMaker

          ImageConfig -> (structure)
              Specifies whether the model container is in Amazon ECR or a pri-
              vate Docker registry accessible from your Amazon Virtual Private
              Cloud  (VPC). For information about storing containers in a pri-
              vate Docker registry, see Use  a  Private  Docker  Registry  for
              Real-Time Inference Containers

              RepositoryAccessMode -> (string)
                 Set this to one of the following values:

                 o Platform - The model image is hosted in Amazon ECR.

                 o Vpc  -  The  model image is hosted in a private Docker reg-
                   istry in your VPC.

              RepositoryAuthConfig -> (structure)
                 (Optional) Specifies an authentication configuration for  the
                 private  docker  registry  where  your model image is hosted.
                 Specify a value for this property only if you  specified  Vpc
                 as the value for the RepositoryAccessMode field, and the pri-
                 vate Docker registry where the model image is hosted requires
                 authentication.

                 RepositoryCredentialsProviderArn -> (string)
                     The  Amazon Resource Name (ARN) of an Amazon Web Services
                     Lambda function that provides credentials to authenticate
                     to  the private Docker registry where your model image is
                     hosted. For information about how to create an Amazon Web
                     Services  Lambda  function,  see Create a Lambda function
                     with the console in the Amazon Web Services Lambda Devel-
                     oper Guide .

          Mode -> (string)
              Whether the container hosts a single model or multiple models.

          ModelDataUrl -> (string)
              The  S3  path where the model artifacts, which result from model
              training, are stored. This path must point to a single gzip com-
              pressed  tar  archive  (.tar.gz suffix). The S3 path is required
              for SageMaker built-in algorithms, but not if you use  your  own
              algorithms.  For  more  information  on built-in algorithms, see
              Common Parameters .

              NOTE:
                 The model artifacts must be in an S3 bucket that  is  in  the
                 same region as the model or endpoint you are creating.

              If you provide a value for this parameter, SageMaker uses Amazon
              Web Services Security Token Service to download model  artifacts
              from  the  S3 path you provide. Amazon Web Services STS is acti-
              vated in your IAM user account by default. If you previously de-
              activated  Amazon Web Services STS for a region, you need to re-
              activate Amazon Web Services STS for that region. For  more  in-
              formation,  see  Activating and Deactivating Amazon Web Services
              STS in an Amazon Web Services Region in the Amazon Web  Services
              Identity and Access Management User Guide .

              WARNING:
                 If  you use a built-in algorithm to create a model, SageMaker
                 requires that you provide a S3 path to the model artifacts in
                 ModelDataUrl .

          Environment -> (map)
              The  environment  variables to set in the Docker container. Each
              key and value in the Environment string to string map  can  have
              length of up to 1024. We support up to 16 entries in the map.

              key -> (string)

              value -> (string)

          ModelPackageName -> (string)
              The  name  or Amazon Resource Name (ARN) of the model package to
              use to create the model.

          InferenceSpecificationName -> (string)
              The inference specification name in the model package version.

          MultiModelConfig -> (structure)
              Specifies additional configuration for multi-model endpoints.

              ModelCacheSetting -> (string)
                 Whether to cache models for a multi-model  endpoint.  By  de-
                 fault,  multi-model  endpoints  cache  models so that a model
                 does not have to be loaded into memory each time  it  is  in-
                 voked.  Some use cases do not benefit from model caching. For
                 example, if an endpoint hosts a large number of  models  that
                 are  each  invoked  infrequently,  the endpoint might perform
                 better  if  you  disable  model  caching.  To  disable  model
                 caching, set the value of this parameter to Disabled .

       Containers -> (list)
          The containers in the inference pipeline.

          (structure)
              Describes the container, as part of model definition.

              ContainerHostname -> (string)
                 This parameter is ignored for models that contain only a Pri-
                 maryContainer .

                 When a ContainerDefinition is part of an inference  pipeline,
                 the  value of the parameter uniquely identifies the container
                 for the purposes of logging and metrics. For information, see
                 Use  Logs  and  Metrics to Monitor an Inference Pipeline . If
                 you don't specify a value for this parameter for  a  Contain-
                 erDefinition  that is part of an inference pipeline, a unique
                 name is automatically assigned based on the position  of  the
                 ContainerDefinition  in  the pipeline. If you specify a value
                 for the ContainerHostName for any ContainerDefinition that is
                 part  of  an inference pipeline, you must specify a value for
                 the ContainerHostName parameter of every  ContainerDefinition
                 in that pipeline.

              Image -> (string)
                 The  path  where inference code is stored. This can be either
                 in Amazon EC2 Container Registry or in a Docker registry that
                 is  accessible  from the same VPC that you configure for your
                 endpoint. If you are using your own custom algorithm  instead
                 of  an  algorithm  provided  by SageMaker, the inference code
                 must meet SageMaker  requirements.  SageMaker  supports  both
                 registry/repository[:tag]   and  registry/repository[@digest]
                 image path formats. For more information, see Using Your  Own
                 Algorithms with Amazon SageMaker

              ImageConfig -> (structure)
                 Specifies  whether  the model container is in Amazon ECR or a
                 private Docker registry accessible from your  Amazon  Virtual
                 Private Cloud (VPC). For information about storing containers
                 in a private Docker registry, see Use a Private  Docker  Reg-
                 istry for Real-Time Inference Containers

                 RepositoryAccessMode -> (string)
                     Set this to one of the following values:

                     o Platform - The model image is hosted in Amazon ECR.

                     o Vpc  -  The  model  image is hosted in a private Docker
                       registry in your VPC.

                 RepositoryAuthConfig -> (structure)
                     (Optional) Specifies an authentication configuration  for
                     the  private  docker  registry  where your model image is
                     hosted. Specify a value for this  property  only  if  you
                     specified  Vpc  as the value for the RepositoryAccessMode
                     field, and the private Docker registry  where  the  model
                     image is hosted requires authentication.

                     RepositoryCredentialsProviderArn -> (string)
                        The  Amazon  Resource Name (ARN) of an Amazon Web Ser-
                        vices Lambda function that provides credentials to au-
                        thenticate  to  the private Docker registry where your
                        model image is hosted. For information  about  how  to
                        create  an  Amazon  Web  Services Lambda function, see
                        Create a Lambda function with the console in the  Ama-
                        zon Web Services Lambda Developer Guide .

              Mode -> (string)
                 Whether  the  container hosts a single model or multiple mod-
                 els.

              ModelDataUrl -> (string)
                 The S3 path where the  model  artifacts,  which  result  from
                 model  training, are stored. This path must point to a single
                 gzip compressed tar archive (.tar.gz suffix). The S3 path  is
                 required  for  SageMaker  built-in algorithms, but not if you
                 use your own algorithms. For more information on built-in al-
                 gorithms, see Common Parameters .

                 NOTE:
                     The  model  artifacts  must be in an S3 bucket that is in
                     the same region as the model or endpoint you  are  creat-
                     ing.

                 If  you  provide  a  value for this parameter, SageMaker uses
                 Amazon Web Services Security Token Service to download  model
                 artifacts  from  the S3 path you provide. Amazon Web Services
                 STS is activated in your IAM user account by default. If  you
                 previously  deactivated Amazon Web Services STS for a region,
                 you need to reactivate Amazon Web Services STS for  that  re-
                 gion.  For  more information, see Activating and Deactivating
                 Amazon Web Services STS in an Amazon Web Services  Region  in
                 the  Amazon  Web Services Identity and Access Management User
                 Guide .

                 WARNING:
                     If you use a built-in algorithm to create a model,  Sage-
                     Maker  requires  that  you provide a S3 path to the model
                     artifacts in ModelDataUrl .

              Environment -> (map)
                 The environment variables to set  in  the  Docker  container.
                 Each  key  and  value in the Environment string to string map
                 can have length of up to 1024. We support up to 16 entries in
                 the map.

                 key -> (string)

                 value -> (string)

              ModelPackageName -> (string)
                 The  name  or Amazon Resource Name (ARN) of the model package
                 to use to create the model.

              InferenceSpecificationName -> (string)
                 The inference specification name in the  model  package  ver-
                 sion.

              MultiModelConfig -> (structure)
                 Specifies additional configuration for multi-model endpoints.

                 ModelCacheSetting -> (string)
                     Whether  to  cache  models for a multi-model endpoint. By
                     default, multi-model endpoints cache  models  so  that  a
                     model does not have to be loaded into memory each time it
                     is invoked. Some use cases  do  not  benefit  from  model
                     caching. For example, if an endpoint hosts a large number
                     of models that are each invoked  infrequently,  the  end-
                     point  might perform better if you disable model caching.
                     To disable model caching, set the value of this parameter
                     to Disabled .

       InferenceExecutionConfig -> (structure)
          Specifies  details  of  how containers in a multi-container endpoint
          are called.

          Mode -> (string)
              How containers in a multi-container are run. The following  val-
              ues are valid.

              o SERIAL - Containers run as a serial pipeline.

              o DIRECT  -  Only  the  individual container that you specify is
                run.

       ExecutionRoleArn -> (string)
          The Amazon Resource Name (ARN) of the IAM role  that  you  specified
          for the model.

       VpcConfig -> (structure)
          A   VpcConfig  object that specifies the VPC that this model has ac-
          cess to. For more information, see Protect  Endpoints  by  Using  an
          Amazon Virtual Private Cloud

          SecurityGroupIds -> (list)
              The VPC security group IDs, in the form sg-xxxxxxxx. Specify the
              security groups for the VPC that is  specified  in  the  Subnets
              field.

              (string)

          Subnets -> (list)
              The  ID  of  the subnets in the VPC to which you want to connect
              your training job or model. For information about the availabil-
              ity of specific instance types, see Supported Instance Types and
              Availability Zones .

              (string)

       CreationTime -> (timestamp)
          A timestamp that shows when the model was created.

       ModelArn -> (string)
          The Amazon Resource Name (ARN) of the model.

       EnableNetworkIsolation -> (boolean)
          If True , no inbound or outbound network calls can  be  made  to  or
          from the model container.



                                                              DESCRIBE-MODEL()
