CREATE-DATA-SOURCE-FROM-S3()                      CREATE-DATA-SOURCE-FROM-S3()



NAME
       create-data-source-from-s3 -

DESCRIPTION
       Creates  a  DataSource object. A DataSource references data that can be
       used to perform CreateMLModel , CreateEvaluation ,  or  CreateBatchPre-
       diction operations.
          CreateDataSourceFromS3  is an asynchronous operation. In response to
          CreateDataSourceFromS3 , Amazon Machine Learning (Amazon ML) immedi-
          ately  returns and sets the DataSource status to PENDING . After the
          DataSource has been created and is ready for use, Amazon ML sets the
          Status parameter to COMPLETED . DataSource in the COMPLETED or PEND-
          ING state can be used to perform only CreateMLModel ,  CreateEvalua-
          tion or CreateBatchPrediction operations.

       If  Amazon ML can't accept the input source, it sets the Status parame-
       ter to FAILED and includes an error message in the Message attribute of
       the GetDataSource operation response.

       The  observation data used in a DataSource should be ready to use; that
       is, it should have a consistent  structure,  and  missing  data  values
       should be kept to a minimum. The observation data must reside in one or
       more .csv files in an Amazon Simple Storage Service (Amazon  S3)  loca-
       tion,  along  with  a  schema that describes the data items by name and
       type. The same schema must be used for all of the data files referenced
       by the DataSource .

       After the DataSource has been created, it's ready to use in evaluations
       and batch predictions. If you plan to use the DataSource  to  train  an
       MLModel  ,  the  DataSource also needs a recipe. A recipe describes how
       each input variable will be used in training  an  MLModel  .  Will  the
       variable  be  included  or excluded from training? Will the variable be
       manipulated; for example, will it be combined with another variable  or
       will  it be split apart into word combinations? The recipe provides an-
       swers to these questions.

       See also: AWS API Documentation

SYNOPSIS
            create-data-source-from-s3
          --data-source-id <value>
          [--data-source-name <value>]
          --data-spec <value>
          [--compute-statistics | --no-compute-statistics]
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --data-source-id (string)
          A user-supplied identifier that uniquely identifies the DataSource .

       --data-source-name (string)
          A user-supplied name or description of the DataSource .

       --data-spec (structure)
          The data specification of a DataSource :

          o DataLocationS3 - The Amazon S3 location of the observation data.

          o DataSchemaLocationS3 - The Amazon S3 location of the DataSchema .

          o DataSchema - A JSON string representing the schema.  This  is  not
            required if DataSchemaUri is specified.

          o DataRearrangement  -  A  JSON string that represents the splitting
            and rearrangement requirements for  the  Datasource  .   Sample  -
            "{\"splitting\":{\"percentBegin\":10,\"percentEnd\":60}}"

          DataLocationS3 -> (string)
              The  location of the data file(s) used by a DataSource . The URI
              specifies a data file or an Amazon Simple Storage Service  (Ama-
              zon S3) directory or bucket containing data files.

          DataRearrangement -> (string)
              A  JSON  string  that represents the splitting and rearrangement
              processing to be applied to a DataSource . If the DataRearrange-
              ment parameter is not provided, all of the input data is used to
              create the Datasource .

              There are multiple parameters that control what data is used  to
              create a datasource:

              o

                **
                percentBegin  **    Use percentBegin to indicate the beginning
                of the range of the data used to create the Datasource. If you
                do  not  include  percentBegin  and percentEnd , Amazon ML in-
                cludes all of the data when creating the datasource.

              System Message: WARNING/2 (<string>:, line 130)
                Inline strong start-string without end-string.

              o

                **
                percentEnd **   Use percentEnd to  indicate  the  end  of  the
                range of the data used to create the Datasource. If you do not
                include percentBegin and percentEnd , Amazon ML  includes  all
                of the data when creating the datasource.

              System Message: WARNING/2 (<string>:, line 132)
                Inline strong start-string without end-string.

              o

                **
                complement  **    The complement parameter instructs Amazon ML
                to use the data that is not included in the range of  percent-
                Begin to percentEnd to create a datasource. The complement pa-
                rameter is useful if you need to  create  complementary  data-
                sources for training and evaluation. To create a complementary
                datasource, use the same  values  for  percentBegin  and  per-
                centEnd  ,  along  with the complement parameter. For example,
                the following two datasources do not share any data,  and  can
                be  used  to  train and evaluate a model. The first datasource
                has 25 percent of the data, and the second one has 75  percent
                of  the  data.  Datasource for evaluation: {"splitting":{"per-
                centBegin":0,  "percentEnd":25}}    Datasource  for  training:
                {"splitting":{"percentBegin":0,    "percentEnd":25,   "comple-
                ment":"true"}}

              System Message: WARNING/2 (<string>:, line 134)
                Inline strong start-string without end-string.

              o

                **
                strategy **   To change how Amazon ML splits the  data  for  a
                datasource,  use the strategy parameter. The default value for
                the strategy parameter is sequential , meaning that Amazon  ML
                takes  all  of  the  data records between the percentBegin and
                percentEnd parameters for the datasource, in  the  order  that
                the  records  appear  in  the  input  data.  The following two
                DataRearrangement lines are examples of  sequentially  ordered
                training  and  evaluation  datasources: Datasource for evalua-
                tion:    {"splitting":{"percentBegin":70,    "percentEnd":100,
                "strategy":"sequential"}}    Datasource for training: {"split-
                ting":{"percentBegin":70,  "percentEnd":100,   "strategy":"se-
                quential", "complement":"true"}}   To randomly split the input
                data into the proportions indicated by  the  percentBegin  and
                percentEnd  parameters,  set  the strategy parameter to random
                and provide a string that is used as the seed  value  for  the
                random data splitting (for example, you can use the S3 path to
                your data as the random seed string). If you choose the random
                split   strategy,  Amazon  ML  assigns  each  row  of  data  a
                pseudo-random number between 0 and 100, and then  selects  the
                rows  that  have  an  assigned number between percentBegin and
                percentEnd . Pseudo-random numbers are assigned using both the
                input  seed  string  value  and  the byte offset as a seed, so
                changing the data results in a different split.  Any  existing
                ordering  is  preserved. The random splitting strategy ensures
                that variables in the training and evaluation  data  are  dis-
                tributed  similarly. It is useful in the cases where the input
                data may have an implicit sort order,  which  would  otherwise
                result  in  training  and  evaluation  datasources  containing
                non-similar data records. The following two  DataRearrangement
                lines  are  examples  of non-sequentially ordered training and
                evaluation datasources: Datasource  for  evaluation:  {"split-
                ting":{"percentBegin":70,  "percentEnd":100,  "strategy":"ran-
                dom", "randomSeed"="s3://my_s3_path/bucket/file.csv"}}   Data-
                source  for  training:  {"splitting":{"percentBegin":70, "per-
                centEnd":100,          "strategy":"random",           "random-
                Seed"="s3://my_s3_path/bucket/file.csv", "complement":"true"}}

              System Message: WARNING/2 (<string>:, line 136)
                Inline strong start-string without end-string.

          DataSchema -> (string)
              A  JSON string that represents the schema for an Amazon S3 Data-
              Source . The DataSchema defines the structure of the observation
              data in the data file(s) referenced in the DataSource .

              You  must  provide  either the DataSchema or the DataSchemaLoca-
              tionS3 .

              Define your DataSchema as  a  series  of  key-value  pairs.  at-
              tributes  and  excludedVariableNames  have an array of key-value
              pairs for their value. Use the following format to  define  your
              DataSchema .

              { "version": "1.0",

              "recordAnnotationFieldName": "F1",

              "recordWeightFieldName": "F2",

              "targetFieldName": "F3",

              "dataFormat": "CSV",

              "dataFileContainsHeader": true,

              "attributes": [

              { "fieldName": "F1", "fieldType": "TEXT" }, { "fieldName": "F2",
              "fieldType": "NUMERIC"  },  {  "fieldName":  "F3",  "fieldType":
              "CATEGORICAL"  }, { "fieldName": "F4", "fieldType": "NUMERIC" },
              { "fieldName": "F5", "fieldType":  "CATEGORICAL"  },  {  "field-
              Name": "F6", "fieldType": "TEXT" }, { "fieldName": "F7", "field-
              Type": "WEIGHTED_INT_SEQUENCE" }, { "fieldName":  "F8",  "field-
              Type": "WEIGHTED_STRING_SEQUENCE" } ],

              "excludedVariableNames": [ "F6" ] }

          DataSchemaLocationS3 -> (string)
              Describes the schema location in Amazon S3. You must provide ei-
              ther the DataSchema or the DataSchemaLocationS3 .

       Shorthand Syntax:

          DataLocationS3=string,DataRearrangement=string,DataSchema=string,DataSchemaLocationS3=string

       JSON Syntax:

          {
            "DataLocationS3": "string",
            "DataRearrangement": "string",
            "DataSchema": "string",
            "DataSchemaLocationS3": "string"
          }

       --compute-statistics | --no-compute-statistics (boolean)
          The compute statistics for a DataSource . The statistics are  gener-
          ated  from  the observation data referenced by a DataSource . Amazon
          ML uses the statistics internally during MLModel training. This  pa-
          rameter  must  be  set to true if the DataSourceneeds to be used for
          MLModel training.

       --cli-input-json (string) Performs service operation based on the  JSON
       string  provided. The JSON string follows the format provided by --gen-
       erate-cli-skeleton. If other arguments  are  provided  on  the  command
       line,  the CLI values will override the JSON-provided values. It is not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton  (string)  Prints  a  JSON skeleton to standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. If provided with the value output,  it  validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By  default, the AWS CLI uses SSL when communicating with AWS services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable automatic pagination.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do  not  sign requests. Credentials will not be loaded if this argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The  maximum socket read time in seconds. If the value is set to 0, the
       socket read will be blocking and not timeout. The default value  is  60
       seconds.

       --cli-connect-timeout (int)

       The  maximum  socket connect time in seconds. If the value is set to 0,
       the socket connect will be blocking and not timeout. The default  value
       is 60 seconds.

OUTPUT
       DataSourceId -> (string)
          A  user-supplied  ID  that uniquely identifies the DataSource . This
          value should be identical to the value of the  DataSourceID  in  the
          request.



                                                  CREATE-DATA-SOURCE-FROM-S3()
